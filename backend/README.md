# Backend Chat â€“ AquaMonitor

Este diretÃ³rio contÃ©m o **backend do Assistente Inteligente (chatbot). O backend Ã© responsÃ¡vel por interpretar perguntas do usuÃ¡rio sobre **qualidade da Ã¡gua**, consultar dados de **telemetria no Firestore** e retornar respostas claras e objetivas.

O sistema foi projetado para funcionar com um **modelo de linguagem pequeno (LLM local)**, priorizando **baixo custo computacional, controle total da lÃ³gica e confiabilidade dos resultados**.

---

## VisÃ£o Geral da Arquitetura

* **FastAPI (Python)**: API principal do chatbot.
* **LLM local via Ollama (qwen2:0.5b)**: usada para interpretar a intenÃ§Ã£o das perguntas e gerar respostas humanizadas.
* **Firestore (Firebase)**: base de dados onde ficam armazenadas as leituras de telemetria (pH, temperatura, turbidez, TDS).
* **LÃ³gica determinÃ­stica no backend**:

  * A LLM **nÃ£o calcula valores**.
  * Todas as mÃ©dias, mÃ¡ximos, mÃ­nimos e tendÃªncias sÃ£o calculadas pelo backend.
  * A LLM apenas entende a pergunta e ajuda a formular a resposta final.

---

## Por que usar um modelo de LLM pequeno?

O modelo utilizado neste projeto (**qwen2:0.5b**) Ã© considerado um **modelo de pequeno porte**. Isso significa que:

* Ele consome pouca memÃ³ria RAM (compatÃ­vel com notebooks comuns).
* Pode rodar **localmente**, sem dependÃªncia de serviÃ§os pagos ou conexÃ£o constante com a internet.
* Possui **menor capacidade de generalizaÃ§Ã£o** quando comparado a modelos grandes (como GPT-4 ou Gemini).

Por esse motivo, o chatbot **nÃ£o Ã© genÃ©rico**. Ele foi **especializado** para o domÃ­nio do projeto (qualidade da Ã¡gua), com:

* Perguntas bem definidas.
* IntenÃ§Ãµes claras (Ãºltima leitura, mÃ¡ximo, mÃ­nimo, mÃ©dia, tendÃªncia, faixa ideal).
* Respostas curtas e objetivas.

Em uma soluÃ§Ã£o com modelos maiores (ex.: OpenAI), seria possÃ­vel responder perguntas sobre diversos assuntos. Neste projeto, a escolha do modelo pequeno foi **intencional**, priorizando:

* EspecializaÃ§Ã£o no domÃ­nio.
* Previsibilidade das respostas.
* Menor custo e maior controle da aplicaÃ§Ã£o.

---

## Estrutura do DiretÃ³rio

```text
backend/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # Endpoint /chat e orquestraÃ§Ã£o geral
â”‚   â”œâ”€â”€ config.py               # ConfiguraÃ§Ãµes via variÃ¡veis de ambiente
â”‚   â”œâ”€â”€ models.py               # Modelos Pydantic e tipos de intenÃ§Ãµes
â”‚   â”œâ”€â”€ firestore_client.py     # ConexÃ£o com o Firestore
â”‚   â”œâ”€â”€ telemetry_repository.py # Consultas e cÃ¡lculos sobre telemetria
â”‚   â”‚
â”‚   â””â”€â”€ llm/
â”‚       â”œâ”€â”€ intent_agent.py     # InterpretaÃ§Ã£o de intenÃ§Ã£o da pergunta
â”‚       â”œâ”€â”€ answer_agent.py     # Respostas para ajuda geral
â”‚       â”œâ”€â”€ prompts.py          # Loader de prompts
â”‚       â””â”€â”€ prompt_water_assistant.txt
â”‚
â”œâ”€â”€ requirements.txt            # DependÃªncias Python
â”œâ”€â”€ .env                        # VariÃ¡veis de ambiente (nÃ£o versionar)
â””â”€â”€ README.md                   # Este arquivo
```

---

## PrÃ©-requisitos

Antes de iniciar, certifique-se de ter instalado:

* **Python 3.10+**
* **pip**
* **Git**
* **Ollama** (para rodar a LLM local)

---

## 1. Clonando o RepositÃ³rio

```bash
git clone <URL_DO_REPOSITORIO>
cd aqua-monitor/backend
```

---

## 2. Criando o Ambiente Virtual (venv)

```bash
python -m venv venv
source venv/bin/activate   # Linux / macOS
# ou
venv\\Scripts\\activate      # Windows
```

---

## 3. Instalando DependÃªncias Python

```bash
pip install -r requirements.txt
```

---

## 4. Instalando e Configurando a LLM (Ollama)

### 4.1. Instalar o Ollama

Acesse:

ğŸ‘‰ [https://ollama.com](https://ollama.com)

E siga as instruÃ§Ãµes para seu sistema operacional.

### 4.2. Baixar o modelo utilizado

```bash
ollama pull qwen2:0.5b
```

### 4.3. Verificar se o Ollama estÃ¡ rodando

```bash
ollama list
```

O serviÃ§o do Ollama roda, por padrÃ£o, em:

```text
http://localhost:11434
```

---

## 5. ConfiguraÃ§Ã£o do Firestore (Firebase)

### 5.1. Criar credenciais no Firebase

1. Acesse o **Firebase Console**.
2. VÃ¡ em **ConfiguraÃ§Ãµes do Projeto â†’ Contas de ServiÃ§o**.
3. Clique em **Gerar nova chave privada**.
4. Baixe o arquivo JSON.

### 5.2. Salvar o JSON no projeto

Coloque o arquivo em um local seguro, por exemplo:

```text
backend/tcc-firebase-admin.json
```

### 5.3. Configurar variÃ¡veis de ambiente

Crie um arquivo `.env` no diretÃ³rio `backend/`:

```env
# Caminho do JSON de credenciais do Firebase/Firestore
GOOGLE_APPLICATION_CREDENTIALS=/home/mateus/Projetos/aqua-monitor/backend/tcc-firebase-admin.json

# ID do projeto Firestore
FIRESTORE_PROJECT_ID=monitor-viveiro

# Nome da coleÃ§Ã£o de telemetria
FIRESTORE_TELEMETRY_COLLECTION=telemetry

# ConfiguraÃ§Ã£o do Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL_NAME=qwen2:0.5b
```

> âš ï¸ **Nunca versionar o arquivo `.env` nem o JSON de credenciais.**

---

## 6. Executando o Backend

```bash
uvicorn app.main:app --reload
```

A API ficarÃ¡ disponÃ­vel em:

```text
http://127.0.0.1:8000
```

DocumentaÃ§Ã£o interativa:

```text
http://127.0.0.1:8000/docs
```

---

## 7. Endpoint Principal

### POST `/chat`

Exemplo de requisiÃ§Ã£o:

```json
{
  "session_id": "123",
  "message": "Qual foi a Ãºltima temperatura aferida?",
  "device_id": "esp32-agua-01",
  "site_id": "fazenda-x_rio-igarape"
}
```

O backend interpreta a pergunta, consulta o Firestore e retorna uma resposta curta e objetiva.

---

## ConsideraÃ§Ãµes Finais

Este backend foi projetado para:

* Ser **especialista em qualidade da Ã¡gua**.
* Trabalhar com **LLMs pequenas**, de forma eficiente.
* Garantir **confiabilidade**, evitando que o modelo invente dados.

Essa abordagem Ã© ideal para projetos acadÃªmicos, sistemas embarcados, IoT e aplicaÃ§Ãµes onde **controle e previsibilidade** sÃ£o mais importantes que generalidade.
